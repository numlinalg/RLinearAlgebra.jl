@article{ailon2009fast,
  title = {The {{Fast Johnson}}--{{Lindenstrauss Transform}} and {{Approximate Nearest Neighbors}}},
  author = {Ailon, Nir and Chazelle, Bernard},
  year = {2009},
  month = jan,
  journal = {SIAM Journal on Computing},
  volume = {39},
  number = {1},
  pages = {302--322},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/060673096},
  urldate = {2025-04-26},
  langid = {english},
  file = {/Users/nathanielpritchard/Zotero/storage/WKB6MTMQ/Ailon and Chazelle - 2009 - The Fast Johnsonâ€“Lindenstrauss Transform and Approximate Nearest Neighbors.pdf}
}

@article{halko2011finding,
  title = {Finding {{Structure}} with {{Randomness}}: {{Probabilistic Algorithms}} for {{Constructing Approximate Matrix Decompositions}}},
  shorttitle = {Finding {{Structure}} with {{Randomness}}},
  author = {Halko, N. and Martinsson, P. G. and Tropp, J. A.},
  year = {2011},
  month = jan,
  journal = {SIAM Review},
  volume = {53},
  number = {2},
  pages = {217--288},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/090771806},
  urldate = {2025-01-24},
  langid = {english}
}

@misc{martinsson2020randomized,
  title = {Randomized {{Numerical Linear Algebra}}: {{Foundations}} \&amp; {{Algorithms}}},
  shorttitle = {Randomized {{Numerical Linear Algebra}}},
  author = {Martinsson, Per-Gunnar and Tropp, Joel},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2002.01387},
  urldate = {2025-02-10},
  abstract = {This survey describes probabilistic algorithms for linear algebra computations, such as factorizing matrices and solving linear systems. It focuses on techniques that have a proven track record for real-world problem instances. The paper treats both the theoretical foundations of the subject and the practical computational issues. Topics covered include norm estimation; matrix approximation by sampling; structured and unstructured random embeddings; linear regression problems; low-rank approximation; subspace iteration and Krylov methods; error estimation and adaptivity; interpolatory and CUR factorizations; Nystr{\"o}m approximation of positive-semidefinite matrices; single view ("streaming") algorithms; full rank-revealing factorizations; solvers for linear systems; and approximation of kernel matrices that arise in machine learning and in scientific computing.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Mathematics,Numerical Analysis (math.NA)}
}

@article{motzkin1954relaxation,
  title = {The {{Relaxation Method}} for {{Linear Inequalities}}},
  author = {Motzkin, T. S. and Schoenberg, I. J.},
  year = {1954},
  journal = {Canadian Journal of Mathematics},
  volume = {6},
  pages = {393--404},
  issn = {0008-414X, 1496-4279},
  doi = {10.4153/CJM-1954-038-x},
  urldate = {2025-08-17},
  abstract = {Let A be a closed set of points in the n-dimensional euclidean space E               n               . If               p               and               p               1               are points of E               n               such that                                                          1.1                                                                         then               p               1               is said to be               point-wise closer               than               p               to the set               A.               If               p               is such that there is no point p               1               which is point-wise closer than               p               to               A               , then               p               is called a               closest point               to the set               A.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/Users/nathanielpritchard/Zotero/storage/AYPL23PN/Motzkin and Schoenberg - 1954 - The Relaxation Method for Linear Inequalities.pdf}
}

@article{needell2014paved,
  title = {Paved with Good Intentions: {{Analysis}} of a Randomized Block {{Kaczmarz}} Method},
  shorttitle = {Paved with Good Intentions},
  author = {Needell, Deanna and Tropp, Joel A.},
  year = {2014},
  month = jan,
  journal = {Linear Algebra and its Applications},
  volume = {441},
  pages = {199--221},
  issn = {00243795},
  doi = {10.1016/j.laa.2012.12.022},
  urldate = {2025-06-06},
  langid = {english},
  file = {/Users/nathanielpritchard/Zotero/storage/FPYFBHCI/Needell and Tropp - 2014 - Paved with good intentions Analysis of a randomized block Kaczmarz method.pdf}
}

@article{patel2023randomized,
  title = {Randomized {{Block Adaptive Linear System Solvers}}},
  author = {Patel, Vivak and Jahangoshahi, Mohammad and Maldonado, D. Adrian},
  year = {2023},
  month = sep,
  journal = {SIAM Journal on Matrix Analysis and Applications},
  volume = {44},
  number = {3},
  pages = {1349--1369},
  issn = {0895-4798, 1095-7162},
  doi = {10.1137/22M1488715},
  urldate = {2025-03-25},
  langid = {english},
  file = {/Users/nathanielpritchard/Zotero/storage/A26Y639N/Patel et al. - 2023 - Randomized Block Adaptive Linear System Solvers.pdf}
}

@misc{pilanci2014iterative,
  title = {Iterative {{Hessian}} Sketch: {{Fast}} and Accurate Solution Approximation for Constrained Least-Squares},
  shorttitle = {Iterative {{Hessian}} Sketch},
  author = {Pilanci, Mert and Wainwright, Martin J.},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1411.0347},
  urldate = {2025-07-08},
  abstract = {We study randomized sketching methods for approximately solving least-squares problem with a general convex constraint. The quality of a least-squares approximation can be assessed in different ways: either in terms of the value of the quadratic objective function (cost approximation), or in terms of some distance measure between the approximate minimizer and the true minimizer (solution approximation). Focusing on the latter criterion, our first main result provides a general lower bound on any randomized method that sketches both the data matrix and vector in a least-squares problem; as a surprising consequence, the most widely used least-squares sketch is sub-optimal for solution approximation. We then present a new method known as the iterative Hessian sketch, and show that it can be used to obtain approximations to the original least-squares problem using a projection dimension proportional to the statistical complexity of the least-squares minimizer, and a logarithmic number of iterations. We illustrate our general theory with simulations for both unconstrained and constrained versions of least-squares, including \${\textbackslash}ell\_1\$-regularization and nuclear norm constraints. We also numerically demonstrate the practicality of our approach in a real face expression classification experiment.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {FOS: Computer and information sciences,FOS: Mathematics,Information Theory (cs.IT),Machine Learning (cs.LG),Machine Learning (stat.ML),Optimization and Control (math.OC)}
}

@article{pritchard2023practical,
  title = {Towards {{Practical Large-Scale Randomized Iterative Least Squares Solvers}} through {{Uncertainty Quantification}}},
  author = {Pritchard, Nathaniel and Patel, Vivak},
  year = {2023},
  month = sep,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {11},
  number = {3},
  pages = {996--1024},
  issn = {2166-2525},
  doi = {10.1137/22M1515057},
  urldate = {2025-01-24},
  langid = {english}
}

@article{pritchard2024solving,
  title = {Solving, Tracking and Stopping Streaming Linear Inverse Problems},
  author = {Pritchard, Nathaniel and Patel, Vivak},
  year = {2024},
  month = aug,
  journal = {Inverse Problems},
  volume = {40},
  number = {8},
  pages = {085003},
  issn = {0266-5611, 1361-6420},
  doi = {10.1088/1361-6420/ad5583},
  urldate = {2025-01-24},
  abstract = {Abstract              In large-scale applications including medical imaging, collocation differential equation solvers, and estimation with differential privacy, the underlying linear inverse problem can be reformulated as a streaming problem. In theory, the streaming problem can be effectively solved using memory-efficient, exponentially-converging streaming solvers. In special cases when the underlying linear inverse problem is finite-dimensional, streaming solvers can periodically evaluate the residual norm at a substantial computational cost. When the underlying system is infinite dimensional, streaming solver can only access noisy estimates of the residual. While such noisy estimates are computationally efficient, they are useful only when their accuracy is known. In this work, we rigorously develop a general family of computationally-practical residual estimators and their uncertainty sets for streaming solvers, and we demonstrate the accuracy of our methods on a number of large-scale linear problems. Thus, we further enable the practical use of streaming solvers for important classes of linear inverse problems.}
}

@article{strohmer2009randomized,
  title = {A {{Randomized Kaczmarz Algorithm}} with {{Exponential Convergence}}},
  author = {Strohmer, Thomas and Vershynin, Roman},
  year = {2009},
  month = apr,
  journal = {Journal of Fourier Analysis and Applications},
  volume = {15},
  number = {2},
  pages = {262--278},
  issn = {1069-5869, 1531-5851},
  doi = {10.1007/s00041-008-9030-4},
  urldate = {2025-06-06},
  copyright = {http://www.springer.com/tdm},
  langid = {english}
}
